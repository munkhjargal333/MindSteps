# main.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from huggingface_hub import InferenceClient
import logging
import re
from typing import Optional, List, Dict, Any
import transformers

from config import Config
from prompts import get_prompt, PromptTemplates
from encoding_dsv32 import encode_messages

# –¢–æ—Ö–∏—Ä–≥–æ–æ —à–∞–ª–≥–∞—Ö
Config.validate()

# Logging —Ç–æ—Ö–∏—Ä—É—É–ª–≥–∞
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# HuggingFace Client
client = InferenceClient(token=Config.HF_TOKEN)

# DeepSeek Tokenizer (–≥–ª–æ–±–∞–ª - –Ω—ç–≥ —É–¥–∞–∞ –∞—á–∞–∞–ª–Ω–∞)
try:
    tokenizer = transformers.AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-V3.2")
    logger.info("‚úÖ DeepSeek-V3.2 Tokenizer –∞—á–∞–∞–ª–∞–≥–¥–ª–∞–∞")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Tokenizer –∞—á–∞–∞–ª–∞—Ö –∞–ª–¥–∞–∞: {e}")
    tokenizer = None

# FastAPI –∞–ø–ø
app = FastAPI(
    title=Config.APP_TITLE,
    version=Config.APP_VERSION,
    description="–ú–æ–Ω–≥–æ–ª —Ö—ç–ª –¥—ç—ç—Ä—Ö Prompt Engineering Platform - DeepSeek-V3.2"
)


# ============ MODELS ============
class ChatInput(BaseModel):
    type: str
    message: str
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    top_k: Optional[int] = None
    repetition_penalty: Optional[float] = None
    presence_penalty: Optional[float] = None
    frequency_penalty: Optional[float] = None
    show_thinking: Optional[bool] = False


class ChatHistory(BaseModel):
    messages: List[Dict[str, str]]
    prompt_type: str = "system"
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None


class ModelResponse(BaseModel):
    """–•–∞—Ä–∏—É–ª—Ç—ã–Ω –±“Ø—Ç—ç—Ü"""
    reply: str
    type: str
    model: str
    thinking: Optional[str] = None
    raw: Optional[str] = None
    tokens_used: Optional[int] = None
    metadata: Optional[Dict[str, Any]] = None


# ============ LLM FUNCTION ============
def call_llm(
    prompt: str = None,
    messages: List[Dict[str, str]] = None,
    max_tokens: int = None,
    temperature: float = None,
    # top_p: float = 0.9,
    # top_k: int = 50,
    # repetition_penalty: float = 1.1,
    # presence_penalty: float = 0.2,
    # frequency_penalty: float = 0.0,
    show_thinking: bool = False
) -> Dict[str, Any]:
    """
    –°–∞–π–∂—Ä—É—É–ª—Å–∞–Ω LLM –¥—É—É–¥–ª–∞–≥–∞ - DeepSeek-V3.2 —Ç–æ—Ö–∏—Ä—É—É–ª–≥–∞—Ç–∞–π
    
    Args:
        prompt: –®—É—É–¥ prompt string (—Ö—ç—Ä—ç–≤ messages –±–∞–π—Ö–≥“Ø–π –±–æ–ª)
        messages: Chat —Ç“Ø“Ø—Ö —Ñ–æ—Ä–º–∞—Ç [{"role": "user", "content": "..."}]
        max_tokens: –•–∞—Ä–∏—É–ª—Ç—ã–Ω –¥—ç—ç–¥ —Ö—ç–º–∂—ç—ç
        temperature: –•–∞—Ä–∏—É–ª—Ç—ã–Ω —Å–æ–Ω–≥–æ–ª—Ç—ã–Ω —Ç–æ—Ö–∏—Ä–≥–æ–æ (0.0-2.0)
        top_p: Nucleus sampling (0.0-1.0)
        top_k: Top-K sampling
        repetition_penalty: –î–∞–≤—Ç–∞–ª—Ç—ã–Ω —à–∏–π—Ç–≥—ç–ª
        presence_penalty: –®–∏–Ω—ç —Å—ç–¥—ç–≤ –≥–∞—Ä–≥–∞—Ö —Ç“Ø–ª—Ö—ç—Ü
        frequency_penalty: –î–∞–≤—Ç–∞–º–∂–∏–π–Ω —à–∏–π—Ç–≥—ç–ª
        show_thinking: <think> —Ö—ç—Å–≥–∏–π–≥ –±—É—Ü–∞–∞—Ö —ç—Å—ç—Ö
        
    Returns:
        Dict —Ö–∞—Ä–∏—É–ª—Ç, thinking, –∞–ª–¥–∞–∞ –º—ç–¥—ç—ç–ª—ç–ª
    """
    max_tokens = max_tokens or Config.MAX_TOKENS
    temperature = temperature or Config.TEMPERATURE
    
    # Messages —ç—Å–≤—ç–ª prompt —à–∞–ª–≥–∞—Ö
    if not messages and not prompt:
        return {"success": False, "error": "‚ö†Ô∏è –ú–µ—Å—Å–µ–∂ —ç—Å–≤—ç–ª –ø—Ä–æ–º–ø—Ç —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π"}
    
    # Prompt string –±–æ–ª messages —Ä“Ø“Ø —Ö”©—Ä–≤“Ø“Ø–ª—ç—Ö
    if prompt and not messages:
        messages = [{"role": "user", "content": prompt}]
    
    try:
        # ============ DEEPSEEK TOKENIZATION ============
        # Encode —Ç–æ—Ö–∏—Ä–≥–æ–æ
        encode_config = dict(
            thinking_mode="thinking",        # reasoning –±–ª–æ–∫–∏–π–≥ –∑”©–≤ —É–¥–∏—Ä–¥–∞—Ö
            drop_thinking=(not show_thinking),  # <think> —Ö–∞—Ä—É—É–ª–∞—Ö —ç—Å—ç—Ö
            add_default_bos_token=True       # BOS token –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä –Ω—ç–º—ç—Ö
        )
        
        # Messages ‚Üí Prompt string (DeepSeek —Ñ–æ—Ä–º–∞—Ç)
        encoded_prompt = encode_messages(messages, **encode_config)
        
        # Prompt ‚Üí Tokens (—Ç–æ–æ—Ü–æ–æ–ª–ª—ã–Ω –∑–æ—Ä–∏–ª–≥–æ–æ—Ä)
        tokens_input = None
        if tokenizer:
            tokens_input = tokenizer.encode(encoded_prompt)
            logger.info(f"üìä Input tokens: {len(tokens_input)}")
        
        logger.info(f"üî° Model: {Config.MODEL_NAME}")
        logger.info(f"üîß –¢–æ—Ö–∏—Ä–≥–æ–æ: temp={temperature}, max_tokens={max_tokens}")

        print("Prompt:", messages)
        print("–•—É–≤–∏—Ä–≥–∞—Å–∞–Ω promt", tokens_input)

        # print("Prompt:", len(messages))
        # print("–•—É–≤–∏—Ä–≥–∞—Å–∞–Ω promt", len(tokens_input))
        raw_text = messages[0]["content"]
        raw_tokens = tokenizer.encode(raw_text)
        print("–ê–Ω—Ö–Ω—ã prompt tokens:", len(raw_tokens))
        print("–•—É–≤–∏—Ä–≥–∞—Å–∞–Ω prompt tokens:", len(tokens_input))
        
        # ============ API –î–£–£–î–õ–ê–ì–ê ============
        response = client.chat.completions.create(
            model=Config.MODEL_NAME,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            # top_p=top_p,
            # top_k=top_k,
            # repetition_penalty=repetition_penalty,
            # presence_penalty=presence_penalty,
            # frequency_penalty=frequency_penalty,
            stop=["<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>"],  # DeepSeek EOS
        )
        
        raw_reply = response.choices[0].message.content.strip()
        
        # <think>...</think> —Ö—ç—Å–≥–∏–π–≥ —Å–∞–ª–≥–∞—Ö
        think_match = re.search(r"<think>(.*?)</think>", raw_reply, flags=re.DOTALL)
        thinking = think_match.group(1).strip() if think_match else ""
        
        # –ñ–∏–Ω—Ö—ç–Ω—ç —Ö–∞—Ä–∏—É–ª—Ç (<think> —Ö–∞—Å–∞–∞–¥ “Ø–ª–¥—Å—ç–Ω)
        clean_reply = re.sub(r"<think>.*?</think>", "", raw_reply, flags=re.DOTALL).strip()
        
        # Token —Ç–æ–æ—Ü–æ–æ (output)
        tokens_output = len(raw_reply.split()) if not tokenizer else len(tokenizer.encode(raw_reply))
        tokens_used = (len(tokens_input) if tokens_input else 0) + tokens_output
        
        logger.info(f"‚úÖ –•–∞—Ä–∏—É–ª—Ç –±—ç–ª—ç–Ω: {len(clean_reply)} —Ç—ç–º–¥—ç–≥—Ç, ~{tokens_used} —Ç–æ–∫–µ–Ω")
        
        return {
            "success": True,
            "reply": clean_reply,
            "thinking": thinking if show_thinking else None,
            "raw": raw_reply if show_thinking else None,
            "tokens_used": tokens_used,
            "tokens_input": len(tokens_input) if tokens_input else None,
            "tokens_output": tokens_output
        }
        
    except Exception as e:
        logger.error(f"‚ö†Ô∏è LLM –∞–ª–¥–∞–∞: {str(e)}")
        error_msg = str(e).lower()
        
        # –ê–ª–¥–∞–∞–Ω—ã —Ç”©—Ä–ª”©”©—Ä –∞–Ω–≥–∏–ª–∞—Ö
        if "rate limit" in error_msg:
            error = "‚è±Ô∏è –•—ç—Ç –æ–ª–æ–Ω —Ö“Ø—Å—ç–ª—Ç. –¢“Ø—Ä —Ö“Ø–ª—ç—ç–Ω—ç “Ø“Ø"
        elif "loading" in error_msg or "unavailable" in error_msg:
            error = "‚è≥ Model –∞—á–∞–∞–ª–∞–≥–¥–∞–∂ –±–∞–π–Ω–∞. –î–∞—Ö–∏–Ω –æ—Ä–æ–ª–¥–æ–Ω–æ —É—É"
        elif "token" in error_msg or "unauthorized" in error_msg:
            error = "üîí Token –∞–ª–¥–∞–∞—Ç–∞–π —ç—Å–≤—ç–ª —Ö“Ø—á–∏–Ω–≥“Ø–π"
        elif "timeout" in error_msg:
            error = "‚è∞ Timeout - Model —É–¥–∞–∞–Ω —Ö–∞—Ä–∏—É ”©–≥—á –±–∞–π–Ω–∞"
        else:
            error = f"‚ö†Ô∏è –ê–ª–¥–∞–∞: {str(e)[:300]}"
        
        return {"success": False, "error": error}


# ============ ENDPOINTS ============
@app.get("/")
def root():
    """API “Ø–Ω–¥—Å—ç–Ω –º—ç–¥—ç—ç–ª—ç–ª"""
    return {
        "status": "‚úÖ –ê–∂–∏–ª–ª–∞–∂ –±–∞–π–Ω–∞",
        "version": Config.APP_VERSION,
        "model": Config.MODEL_NAME,
        "description": "–ú–æ–Ω–≥–æ–ª —Ö—ç–ª –¥—ç—ç—Ä—Ö Prompt Engineering API",
        "endpoints": {
            "GET /": "API –º—ç–¥—ç—ç–ª—ç–ª",
            "GET /prompts": "Prompt —Ç”©—Ä–ª“Ø“Ø–¥–∏–π–Ω –∂–∞–≥—Å–∞–∞–ª—Ç",
            "POST /chat": "–ù—ç–≥ —É–¥–∞–∞–≥–∏–π–Ω —á–∞—Ç",
            "POST /chat/history": "–¢“Ø“Ø—Ö—Ç—ç–π —á–∞—Ç (multi-turn)",
            "GET /health": "Health check",
            "GET /model-info": "Model —Ç–æ—Ö–∏—Ä–≥–æ–æ–Ω—ã –º—ç–¥—ç—ç–ª—ç–ª"
        },
        "features": [
            "ü§ñ DeepSeek-V3.2 –∑–∞–≥–≤–∞—Ä",
            "üí≠ Thinking mode –¥—ç–º–∂–ª—ç–≥",
            "üéØ 9 —Ç”©—Ä–ª–∏–π–Ω prompt engineering",
            "üìä –¢–æ—Ö–∏—Ä—É—É–ª–∞—Ö –±–æ–ª–æ–º–∂–∏—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä“Ø“Ø–¥",
            "üí¨ Multi-turn conversation"
        ]
    }


@app.get("/prompts")
def list_prompts():
    """Prompt —Ç”©—Ä–ª“Ø“Ø–¥–∏–π–Ω –∂–∞–≥—Å–∞–∞–ª—Ç"""
    names = PromptTemplates.get_names()
    descriptions = PromptTemplates.get_descriptions()
    
    return {
        "available": list(names.keys()),
        "templates": {k: {"name": v, "description": descriptions.get(k, "")} 
                     for k, v in names.items()},
        "count": len(names),
        "usage": "POST /chat —ç—Å–≤—ç–ª /chat/history –¥—ç—ç—Ä 'type' –ø–∞—Ä–∞–º–µ—Ç—Ä—ç—ç—Ä —Å–æ–Ω–≥–æ"
    }


@app.post("/chat")
def chat(req: ChatInput):
    """
    –ù—ç–≥ —É–¥–∞–∞–≥–∏–π–Ω —á–∞—Ç
    
    """
    
    # Prompt —Ç”©—Ä”©–ª —à–∞–ª–≥–∞—Ö
    available = list(PromptTemplates.get_all().keys())
    if req.type not in available:
        raise HTTPException(
            status_code=400,
            detail=f"‚ö†Ô∏è '{req.type}' –±–∞–π—Ö–≥“Ø–π. –ë–æ–ª–æ–º–∂—Ç–æ–π: {', '.join(available)}"
        )
    
    # –•–æ–æ—Å–æ–Ω –º–µ—Å—Å–µ–∂ —à–∞–ª–≥–∞—Ö
    if not req.message.strip():
        raise HTTPException(status_code=400, detail="‚ö†Ô∏è –•–æ–æ—Å–æ–Ω –º–µ—Å—Å–µ–∂")
    
    # Prompt “Ø“Ø—Å–≥—ç—Ö
    prompt = get_prompt(req.type, req.message)
    logger.info(f"üîé –¢”©—Ä”©–ª: {req.type} | –ú–µ—Å—Å–µ–∂: {req.message[:50]}...")
    
    # LLM –¥—É—É–¥–∞—Ö
    result = call_llm(
        prompt=prompt,
        max_tokens=req.max_tokens,
        temperature=req.temperature,
        # top_p=req.top_p or 0.9,
        # top_k=req.top_k or 50,
        # repetition_penalty=req.repetition_penalty or 1.1,
        # presence_penalty=req.presence_penalty or 0.2,
        # frequency_penalty=req.frequency_penalty or 0.0,
        # show_thinking=req.show_thinking
    )
    
    if not result.get("success"):
        raise HTTPException(status_code=500, detail=result.get("error"))
    
    return ModelResponse(
        reply=result["reply"],
        type=req.type,
        model=Config.MODEL_NAME,
        thinking=result.get("thinking"),
        raw=result.get("raw"),
        tokens_used=result.get("tokens_used"),
        metadata={
            "temperature": req.temperature or Config.TEMPERATURE,
            "max_tokens": req.max_tokens or Config.MAX_TOKENS,
            "tokens_input": result.get("tokens_input"),
            "tokens_output": result.get("tokens_output")
        }
    )


@app.post("/chat/history")
def chat_with_history(req: ChatHistory):
    """
    –¢“Ø“Ø—Ö—Ç—ç–π —á–∞—Ç (multi-turn conversation)
    
    Example:
    ```json
    {
        "messages": [
            {"role": "user", "content": "–°–∞–π–Ω —É—É"},
            {"role": "assistant", "content": "–°–∞–π–Ω –±–∞–π–Ω–∞ —É—É"},
            {"role": "user", "content": "Python –≥—ç–∂ —é—É –≤—ç?"}
        ],
        "prompt_type": "system",
        "temperature": 0.4
    }
    ```
    """
    
    # –ú–µ—Å—Å–µ–∂ —Ñ–æ—Ä–º–∞—Ç —à–∞–ª–≥–∞—Ö
    if not req.messages or len(req.messages) == 0:
        raise HTTPException(status_code=400, detail="‚ö†Ô∏è –ú–µ—Å—Å–µ–∂ —Ö–æ–æ—Å–æ–Ω –±–∞–π–Ω–∞")
    
    # –°“Ø“Ø–ª—á–∏–π–Ω –º–µ—Å—Å–µ–∂ user –±–∞–π—Ö —ë—Å—Ç–æ–π
    if req.messages[-1]["role"] != "user":
        raise HTTPException(
            status_code=400, 
            detail="‚ö†Ô∏è –°“Ø“Ø–ª—á–∏–π–Ω –º–µ—Å—Å–µ–∂ 'user' –±–∞–π—Ö —ë—Å—Ç–æ–π"
        )
    
    # Prompt —Ç”©—Ä”©–ª —à–∞–ª–≥–∞—Ö
    available = list(PromptTemplates.get_all().keys())
    if req.prompt_type not in available:
        raise HTTPException(
            status_code=400,
            detail=f"‚ö†Ô∏è '{req.prompt_type}' –±–∞–π—Ö–≥“Ø–π"
        )
    
    # System prompt –Ω—ç–º—ç—Ö (—Ö—ç—Ä—ç–≤ –±–∞–π—Ö–≥“Ø–π –±–æ–ª)
    if req.messages[0]["role"] != "system":
        system_prompt = PromptTemplates.get_system_for_type(req.prompt_type)
        req.messages.insert(0, {"role": "system", "content": system_prompt})
    
    logger.info(f"üí¨ Multi-turn —á–∞—Ç: {len(req.messages)} –º–µ—Å—Å–µ–∂")
    
    # LLM –¥—É—É–¥–∞—Ö
    result = call_llm(
        messages=req.messages,
        max_tokens=req.max_tokens,
        temperature=req.temperature,
        show_thinking=False
    )
    
    if not result.get("success"):
        raise HTTPException(status_code=500, detail=result.get("error"))
    
    return ModelResponse(
        reply=result["reply"],
        type=req.prompt_type,
        model=Config.MODEL_NAME,
        tokens_used=result.get("tokens_used"),
        metadata={
            "conversation_length": len(req.messages),
            "temperature": req.temperature or Config.TEMPERATURE
        }
    )


@app.get("/model-info")
def model_info():
    """Model —Ç–æ—Ö–∏—Ä–≥–æ–æ–Ω—ã –º—ç–¥—ç—ç–ª—ç–ª"""
    return {
        "model": Config.MODEL_NAME,
        "provider": "HuggingFace Inference API",
        "capabilities": {
            "thinking_mode": True,
            "multi_turn": True,
            "max_tokens": "1-4000 (model-–æ–æ—Å —Ö–∞–º–∞–∞—Ä–Ω–∞)",
            "languages": ["–ú–æ–Ω–≥–æ–ª", "English", "Chinese", "–û–ª–æ–Ω —Ö—ç–ª"]
        },
        "default_parameters": {
            "temperature": Config.TEMPERATURE,
            "max_tokens": Config.MAX_TOKENS,
            "top_p": 0.9,
            "top_k": 50,
            "repetition_penalty": 1.1,
            "presence_penalty": 0.2,
            "frequency_penalty": 0.0
        },
        "prompt_types": list(PromptTemplates.get_all().keys())
    }


@app.get("/health")
def health():
    """Health check endpoint"""
    try:
        # Token —à–∞–ª–≥–∞—Ö
        if not Config.HF_TOKEN:
            return {
                "status": "unhealthy",
                "error": "üîí HF_TOKEN –±–∞–π—Ö–≥“Ø–π",
                "model": Config.MODEL_NAME
            }
        
        return {
            "status": "healthy",
            "model": Config.MODEL_NAME,
            "version": Config.APP_VERSION,
            "timestamp": "2025-12-12"
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }


# ============ ERROR HANDLERS ============
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    """HTTP –∞–ª–¥–∞–∞–Ω—ã handler"""
    logger.error(f"HTTP –∞–ª–¥–∞–∞: {exc.detail}")
    return {
        "error": exc.detail,
        "status_code": exc.status_code
    }


@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """–ï—Ä”©–Ω—Ö–∏–π –∞–ª–¥–∞–∞–Ω—ã handler"""
    logger.error(f"–ï—Ä”©–Ω—Ö–∏–π –∞–ª–¥–∞–∞: {str(exc)}")
    return {
        "error": f"‚ö†Ô∏è –°–µ—Ä–≤–µ—Ä–∏–π–Ω –∞–ª–¥–∞–∞: {str(exc)[:200]}",
        "status_code": 500
    }